{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.datasets import load_digits\nfrom sklearn.decomposition import PCA\nfrom main_link import MainLink\nfrom bs4 import BeautifulSoup\nimport numpy as np",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "main_url = 'https://www.varzesh3.com/'\nvarzesh3 = MainLink('varzesh3', main_url)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "url = 'https://www.varzesh3.com/sitemap/news'\nhtml = varzesh3.get(url)\n\nsoup = BeautifulSoup(html, 'html.parser')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#soup",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "print(\"start\")\ndoc = []\nfor p in soup.find_all('loc'):\n    doc.append(p.text)\n#print(doc)\nlist_news = []\nfor i in range(2000):\n    url = doc[i]\n    html = varzesh3.get(url)\n    soup = BeautifulSoup(html, 'html.parser')\n    news = ''\n    for p in soup.find_all('p'):\n        news += p.text\n    \n    list_news.append(news)\n\nprint(\"finish\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# import csv\n# csv_file_path = 'varzesh3.csv'\n# with open(csv_file_path, 'w') as csvfile:\n#     csv_writer = csv.writer(csvfile)\n#     csv_writer.writerow(['Text', 'Category'])\n#     for khabar in list_news:\n#         csv_writer.writerow([khabar, 'Sport'])\n\n# print(list_news)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "import bs4 as bs\nimport requests\nprint(\"start\")\n# assign URL\nURL = 'https://www.tabnak.ir/fa/archive?service_id=24&sec_id=-1&cat_id=-1&rpp=100&from_date=1384/01/01&to_date=1402/11/09&'\ncount = 1\ndoc = []\nd = \"p=\"+str(count)\nfor i in range(10):\n    d = \"p=\"+str(count)\n    URL_final = URL+d\n    url_link = requests.get(URL_final)\n    file = bs.BeautifulSoup(url_link.text, \"lxml\")\n#     links = file.find_all('div', class_=\"row\")\n    links_d = file.find_all('a', class_=\"title5\")\n    for i in range(100):\n        link = links_d[i].get('href')\n        doc.append(link)\n    count = count + 1\n\nprint(len(doc))\nprint(\"finsh\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "import urllib.parse\n\nmain_url = 'https://www.tabnak.ir'\nprint(doc[0])\nlist_news = []\nprint(\"start\")\nfor i in range(1000):\n    url = main_url + urllib.parse.quote(doc[i])\n    url_link = requests.get(url)\n    soup = bs.BeautifulSoup(url_link.text, \"html.parser\")\n    news = ''\n    for p in soup.find_all('p'):\n        news += p.text\n\n    list_news.append(news)\n\nprint(list_news)\nprint(news)\nprint(\"finish\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "print(\"start\")\nURL = 'https://www.tabnak.ir/fa/archive?service_id=2&sec_id=-1&cat_id=-1&rpp=100&from_date=1384/01/01&to_date=1402/11/09&'\ncount = 1\ndoc2 = []\nd = \"p=\"+str(count)\nfor i in range(10):\n    d = \"p=\"+str(count)\n    URL_final = URL+d\n    url_link = requests.get(URL_final)\n    file = bs.BeautifulSoup(url_link.text, \"lxml\")\n    links_d = file.find_all('a', class_=\"title5\")\n    print(len(links_d))\n    for i in range(100):\n        link = links_d[i].get('href')\n        doc2.append(link)\n    count = count + 1\n\nprint(len(doc2))\nprint(\"finsh\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "main_url = 'https://www.tabnak.ir'\nprint(doc2[0])\nlist2_news = []\nprint(\"start\")\nfor i in range(1000):\n    url = main_url + urllib.parse.quote(doc2[i])\n    url_link = requests.get(url)\n    soup = bs.BeautifulSoup(url_link.text, \"html.parser\")\n    news = ''\n    for p in soup.find_all('p'):\n        news += p.text\n\n    list2_news.append(news)\n\nprint(\"finish\")\nprint(list2_news)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# import csv\n# csv_file_path = 'tabnak.csv'\n# with open(csv_file_path, 'w') as csvfile:\n#     csv_writer = csv.writer(csvfile)\n#     csv_writer.writerow(['Text', 'Category'])\n#     for khabar in list_news:\n#         csv_writer.writerow([khabar, 'Politics'])\n#     for khabar in list2_news:\n#         csv_writer.writerow([khabar, 'Sport'])\n\n# print(list_news)\n# print(list2_news)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "URL = 'https://www.entekhab.ir/fa/archive?service_id=2&sec_id=-1&cat_id=-1&rpp=100&from_date=1389/10/01&to_date=1402/11/09&'\ncount = 1\ndoc = []\nd = \"p=\"+str(count)\nfor i in range(10):\n    d = \"p=\"+str(count)\n    URL_final = URL+d\n    url_link = requests.get(URL_final)\n    file = bs.BeautifulSoup(url_link.text, \"lxml\")\n    links_d = file.find_all('a', class_=\"title5\")\n    for i in range(100):\n        link = links_d[i].get('href')\n        doc.append(link)\n    count = count + 1\n\nprint(len(doc))\nprint(\"finsh\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "main_url = 'https://www.entekhab.ir'\nprint(doc[0])\nlist_news = []\nprint(\"start\")\nfor i in range(1000):\n    url = main_url + urllib.parse.quote(doc[i])\n    url_link = requests.get(url)\n    soup = bs.BeautifulSoup(url_link.text, \"html.parser\")\n    news = ''\n    for p in soup.find_all('p'):\n        news += p.text\n\n    list_news.append(news)\n\nprint(list_news)\nprint(news)\nprint(\"finish\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "URL = 'https://www.entekhab.ir/fa/archive?service_id=9&sec_id=-1&cat_id=-1&rpp=100&from_date=1389/10/01&to_date=1402/11/09&'\ncount = 1\ndoc2 = []\nd = \"p=\"+str(count)\nfor i in range(10):\n    d = \"p=\"+str(count)\n    URL_final = URL+d\n    url_link = requests.get(URL_final)\n    file = bs.BeautifulSoup(url_link.text, \"lxml\")\n    links_d = file.find_all('a', class_=\"title5\")\n    for i in range(100):\n        link = links_d[i].get('href')\n        doc2.append(link)\n    count = count + 1\n\nprint(len(doc2))\nprint(\"finsh\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "main_url = 'https://www.entekhab.ir'\nprint(doc2[0])\nlist2_news = []\nprint(\"start\")\nfor i in range(1000):\n    url = main_url + urllib.parse.quote(doc2[i])\n    url_link = requests.get(url)\n    soup = bs.BeautifulSoup(url_link.text, \"html.parser\")\n    news = ''\n    for p in soup.find_all('p'):\n        news += p.text\n\n    list2_news.append(news)\n\nprint(list2_news)\nprint(news)\nprint(\"finish\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# csv_file_path = 'entekhab.csv'\n# with open(csv_file_path, 'w') as csvfile:\n#     csv_writer = csv.writer(csvfile)\n#     csv_writer.writerow(['Text', 'Category'])\n#     for khabar in list_news:\n#         csv_writer.writerow([khabar, 'Politics'])\n#     for khabar in list2_news:\n#         csv_writer.writerow([khabar, 'Sport'])\n\n# print(list_news)\n# print(list2_news)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}